\documentclass[aspectratio=169]{beamer}
\usetheme{metropolis}
\metroset{subsectionpage=progressbar}

\usefonttheme{professionalfonts} % 使用系统/自定字体


% === 字体设置 ===
\usepackage[UTF8,scheme=plain,fontset=none]{ctex}
\setCJKmainfont{Source Han Serif CN}[BoldFont={Source Han Serif CN Bold}]
\setCJKsansfont{Source Han Sans CN}[BoldFont={Source Han Sans CN Bold}]
% \setCJKmonofont{Sarasa Mono CN}

% \input{commands.tex}

% beamer 已加载 hyperref；加 unicode 以支持中文书签
\hypersetup{unicode}

% define paragraph
\providecommand{\paragraph}[1]{\smallskip\textbf{#1}\par}

% 常用包
\usepackage{longtable,booktabs}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta}
% \graphicspath{{.}{./figs/}{./images/}{./images_in_paper/}}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{svg}
\usepackage{booktabs}
\usepackage{array}
\usepackage{threeparttable}

% 算法环境（与 beamer 兼容）
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}  % 提供 algorithmic 环境、\State 等
% 可选：微调 algorithmic 缩进
\algrenewcommand\algorithmicindent{0.8em}

% 数学粗体与梯度符号
\usepackage{bm} % \boldsymbol
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\grad}{\nabla}
% 超链接（beamer 已加载 hyperref，这里只补选项）
% \hypersetup{unicode=true}

% 编号风格
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{.}

\title{}
\author{分类问题视角下的AdaBoost算法}
\date{\today}

%---Document Begins---
\begin{document}
\begin{frame}[plain]
  \titlepage
\end{frame}

\section{AdaBoost模型结构}

\begin{frame}{adaboost模型介绍}
  % 思想性地描述 AdaBoost 的整体结构
  \begin{itemize}
    \item AdaBoost 是一种典型的提升（Boosting）型集成学习框架，
      其核心思想是：\textbf{通过多轮训练、聚焦难样本，把一群“弱学习器”提升为一个“强学习器”。}
    \item 在每一轮中，AdaBoost 都会为总模型增加一个新的学习器，直到模型的弱学习器个数达到预先指定的值。
    \item 训练新学习器时，根据上一轮的推理结果在同一训练集上\textbf{重新分配样本权重}，
      使新的弱学习器更加关注上一轮中被分错或“难学”的样本。
    \item 各轮得到的弱学习器本身能力都比较弱，但在最后通过加权组合（加权投票或加权求和），
      形成一个整体性能更高、泛化能力更强的强学习器。
    \item AdaBoost 不限定弱学习器的具体形式（如决策树桩、小深度决策树等），因此可以看作一个\textbf{通用的、
      可移植的集成学习框架}。
  \end{itemize}
\end{frame}

\begin{frame}{模型结构图}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{./assets/adaboost1.png}
    \caption{AdaBoost模型结构图}
  \end{figure}
\end{frame}

\begin{frame}{模型计算流程}
  假设训练集包含 $m$ 个样本 $\{(x_i,y_i),\cdots,(x_m,y_m)\}$。
  
  第 $1$ 个学习器直接由初始权重的样本训练即可
  
  不失一般性地，假设现在训练已经进行到第 $t$ 轮。$(t=2,\dots,T)$

  记第 $t-1$ 个学习器在样本 $x$ 上的预测结果为 $h_{t-1}(x)$。

  设训练第 $t-1$ 个弱学习器所使用的样本权重为
  \[
    D_t = \{\omega_{t-1,1},\cdots,\omega_{t-1,m}\},\qquad
    \sum_{i=1}^m \omega_{t-1,i} = 1.
  \]

\end{frame}

\begin{frame}{模型计算流程:学习器权重}
  则第 $t-1$ 轮弱学习器的加权错误率为
  \[
    \varepsilon_{t-1}
      = \sum_{i=1}^m \omega_{t-1,i}\,\mathbf{1}\{h_{t-1}(x_i)\neq y_i\}.
  \]
  第$t-1$个弱学习器的投票权重为
  \[
    alpha_{t-1}=\frac{1}{2}(\frac{1-\varepsilon_{t-1}}{\varepsilon_{t-1}})
  \]
\end{frame}


\begin{frame}{模型计算流程：样本权重更新}
  \begin{enumerate}
    \item 计算系数
      \[
        \beta_{t-1} = \frac{\varepsilon_{t-1}} {1-\varepsilon_{t-1}},
        \qquad 0 < \beta_{t-1} < 1.
      \]
    \item 先得到更新后的未归一化权重
      \[
        \tilde{\omega}_{t,i}
          = \omega_{t-1,i}\,\beta_{t-1}^{\,1-\mathbf{1}\{h_{t-1}(x_i)\neq y_i\}},
          \qquad i=1,\dots,m.
      \]
    \item 再将其归一化，得到第 $t$ 轮的权重分布
      \[
        \omega_{t,i}
          = \frac{\tilde{\omega}_{t,i}}
                  {\sum_{j=1}^m \tilde{\omega}_{t,j}}.
      \]
    \item 由得到的新样本权重训练集训练得到第 $t$ 个弱学习器
  \end{enumerate}

\end{frame}

\begin{frame}{模型计算流程：样本权重更新}
  \textbf{第 $t$ 轮：}
  \textbf{直观理解：}
  \[
    \begin{cases}
      h_{t-1}(x_i)=y_i
      \Rightarrow \tilde{\omega}_{t,i}=\omega_{t-1,i}\,\beta_{t-1}
        & \text{（分对：权重减小）}\\[0.3em]
      h_{t-1}(x_i)\neq y_i
      \Rightarrow \tilde{\omega}_{t,i}=\omega_{t-1,i}
        & \text{（分错：权重不变，归一化后相对增大）}
    \end{cases}
  \]
  这意味这模型会不断给难以分类的样本点增加权重，而这些难分类的点可能是数据中的噪声/离群点，可能导致模型在中后期过拟合。
\end{frame}

\section{收敛性}

\begin{frame}{AdaBoost 的训练误差界}

  \textbf{设：}
  \begin{itemize}
    \item 第 $t$ 轮弱学习器的加权错误率为 $\varepsilon_t$；
    \item 最终强分类器为 $h_f$，其在训练分布 $D$ 下的错误率为
      \[
        \varepsilon = \Pr_{i\sim D}[h_f(x_i)\neq y_i].
      \]
  \end{itemize}

\end{frame}
\begin{frame}{AdaBoost 的训练误差界}
  \textbf{训练误差上界}
  \[
    \varepsilon \;\le\; 2^T \prod_{t=1}^T \sqrt{\varepsilon_t(1-\varepsilon_t)}.
  \]

  引入第 $t$ 轮的“优势”（edge）:比随机猜测强的部分
  \[
    \gamma_t = \tfrac{1}{2}-\varepsilon_t,
  \]
  则上界可以改写为
  \[
    \varepsilon
    \;\le\; \prod_{t=1}^T \sqrt{1-4\gamma_t^2}
    = \exp\!\Bigl(-\sum_{t=1}^T \mathrm{KL}\!\bigl(\tfrac12 \,\big\|\, \tfrac12-\gamma_t\bigr)\Bigr)
    \;\le\; \exp\!\Bigl(-2\sum_{t=1}^T \gamma_t^2\Bigr).
  \]

\end{frame}

\begin{frame}{AdaBoost 的训练误差界}
  \textbf{特殊情形：} 若所有弱学习器的错误率都相同，
  $\varepsilon_t = \tfrac12-\gamma$（$\gamma>0$），则
  \[
    \varepsilon \;\le\; (1-4\gamma^2)^{T/2}
    = \exp\!\bigl(-T\cdot \mathrm{KL}(\tfrac12 \,\|\, \tfrac12-\gamma)\bigr)
    \;\le\; \exp(-2T\gamma^2).
  \]

  \textbf{结论：} 只要每一轮的弱学习器都略好于随机猜测（$\gamma_t>0$），
  AdaBoost 在训练集上的错误率会随轮数 $T$ \alert{指数级下降}。

\end{frame}

\begin{frame}{收敛性证明}
  \textbf{引理：} 训练误差界受限于归一化因子之积。
  \[
    \frac{1}{m}\sum_{i=1}^m \exp(-y_i f(x_i)) = \prod_{t=1}^T Z_t.
  \]
  \[
  Z_t = \sum_{y_i=h_t(x_i)} \omega_{ti} e^{-\alpha_t} + \sum_{y_i \neq h_t(x_i)} \omega_{ti} e^{\alpha_t}
  = (1-\varepsilon_t)e^{-\alpha_t} + \varepsilon_t e^{\alpha_t}
  \]
  通过求导计算得到$Z_t$ 的极小值：
  \[
  Z_t = 2\sqrt{\varepsilon_t(1-\varepsilon_t)} = \sqrt{1-4\gamma_t^2}
  \]
  从而得证：
  \[
  \varepsilon_{train}
  \;\le\; \prod_{t=1}^T \sqrt{1-4\gamma_t^2}
  = \exp\!\Bigl(-\sum_{t=1}^T \mathrm{KL}\!\bigl(\tfrac12 \,\big\|\, \tfrac12-\gamma_t\bigr)\Bigr)
  \;\le\; \exp\!\Bigl(-2\sum_{t=1}^T \gamma_t^2\Bigr).
  \]

\end{frame}

\section{总结与展望}

\begin{frame}{AdaBoost 优缺点总结}
  \begin{columns}[T]
    \column{0.48\textwidth}
      \textbf{优点：}
      \begin{itemize}
        \item \textbf{泛化能力强：} 在许多问题上不易过拟合（Margin 理论）。
        \item \textbf{参数少：} 原始算法几乎无需调参。
        \item \textbf{通用性：} 可与任何弱学习器结合。
      \end{itemize}
    \column{0.48\textwidth}
      \textbf{缺点：}
      \begin{itemize}
        \item \textbf{对噪声敏感：} 异常值权重会被过度放大（本次实验重点验证）。
        \item \textbf{串行训练：} 难以并行化，训练速度较慢。
      \end{itemize}
  \end{columns}
  
  \vspace{1em}
  \textbf{经典应用：} Viola-Jones 人脸检测框架（基于 Haar 特征 + AdaBoost 级联）。
\end{frame}

\section{任务：手写数字识别}

\begin{frame}{任务}
  \textbf{核心目标}:
  
  训练一个AdaBoost分类器,对手写数字图片进行分类。

  这是一个多分类（十类）问题。
\end{frame}

\begin{frame}{数据集及数据处理}
  \textbf{数据集}:
  \begin{itemize}
    \item 使用MNIST数据集，按照8：2切分训练集和测试集，
    \item 在MNIST测试集和课程提供的手写图片两组数据上分别测试。
  \end{itemize}
  \textbf{数据预处理}:
  \begin{itemize}
    \item 所有图片转化为黑底白字
    \item 按照包含该数字的最小正方形进行切割
    \item 使用cv2.resize方法将图片缩放至20x20
    \item 将数字图片嵌入到28x28的纯黑色背景
  \end{itemize}
\end{frame}

\begin{frame}{数据集及数据处理}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{./assets/course_data.png}
    \caption{预处理后数据示意图}
  \end{figure}
\end{frame}

\begin{frame}{数据集及数据处理}
  \textbf{特征提取方法}:
  \begin{itemize}
    \item 原始图片，reshape为(784,)一维向量
    \item HOG 
    \item Hu不变矩
  \end{itemize}
  \textbf{目标}:
  \begin{itemize}
  	\item 对比原始特征和提取特征的聚类效果，证明提取特征的有效性
  	\item 提升风格迁移泛化能力
  \end{itemize}
  \textbf{聚类与可视化}:
  \begin{itemize}
  	\item 对不同风格数据集的特征进行聚类和可视化，呈现分布差异
  \end{itemize}
\end{frame}

\begin{frame}{关键代码}
  使用`sklearn.ensemble.AdaBoostClassifier`实现的AdaBoost分类器
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{./assets/sklearn_AdaBoost.png}
    \caption{sklearn AdaBoost分类器实例构建}
  \end{figure}
\end{frame}

\begin{frame}{关键代码}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{./assets/run.png}
    \caption{实验主入口示例}
  \end{figure}
\end{frame}

\subsection{实验数据展示}
\begin{frame}{AdaBoost参数选择}
  \begin{table}[]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \textbf{参数}          & \textbf{值}            \\ \hline
    \textbf{max\_depth}    & 2                      \\ \hline
    \textbf{max\_features} & 0.2                    \\ \hline
    \textbf{criterion}     & entropy                \\ \hline
    \textbf{random\_state} & 42                     \\ \hline
    \textbf{n\_estimators} & 1000                   \\ \hline
    \textbf{learning\_rate}& 0.5                    \\ \hline
    \end{tabular}
    \caption{训练参数}
  \end{table}
\end{frame}
\begin{frame}{原始图片}
  \begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{数据集}       & \textbf{准确率}    & \textbf{精度 (宏平均)}   & \textbf{召回率 (宏平均)} & \textbf{F1值 (宏平均)} \\ \hline
    \textbf{MNIST}        & 0.8727            & 0.8758                  & 0.8719                  & 0.8723                  \\ \hline
    \textbf{课程数据集}   & 0.4               & 0.2083                  & 0.4                     & 0.2567                  \\ \hline
    \end{tabular}
    \caption{模型在MNIST和课程数据集上的测试结果}
  \end{table}
\end{frame}

\begin{frame}{Hu不变矩}
  \begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{数据集}       & \textbf{准确率}    & \textbf{精度 (宏平均)}   & \textbf{召回率 (宏平均)} & \textbf{F1值 (宏平均)} \\ \hline
    \textbf{MNIST}        & 0.5051            & 0.4921                  & 0.4961                  & 0.4904                  \\ \hline
    \textbf{课程数据集}   & 0.2               & 0.15                    & 0.2                     & 0.1667                  \\ \hline
    \end{tabular}
    \caption{模型在MNIST和课程数据集上的测试结果}
  \end{table}
\end{frame}

\begin{frame}{理想模型数据展示(HOG)}
  \begin{table}[]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \textbf{参数}              & \textbf{值}        \\ \hline
    \textbf{orientations}      & 9                  \\ \hline
    \textbf{pixels\_per\_cell} & [2, 2]             \\ \hline
    \textbf{cells\_per\_block} & [2, 2]             \\ \hline
    \end{tabular}
    \caption{HOG 特征提取参数}
  \end{table}

  \begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{数据集}       & \textbf{准确率}    & \textbf{精度 (宏平均)}   & \textbf{召回率 (宏平均)} & \textbf{F1值 (宏平均)} \\ \hline
    \textbf{MNIST}        & 0.9356            & 0.93596                 & 0.93515                 & 0.93536                 \\ \hline
    \textbf{课程数据集}   & 0.7               & 0.5833                  & 0.7                     & 0.6167                  \\ \hline
    \end{tabular}
    \caption{模型在MNIST和课程数据集上的测试结果}
  \end{table}
\end{frame}

\begin{frame}{理想模型数据展示}
	\begin{tabular}{cc}
		\includegraphics[width=0.45\textwidth]{./assets/best_accuracy.png} &
		\includegraphics[width=0.45\textwidth]{./assets/best_f1_score.png} \\
	\end{tabular}
\end{frame}

\subsection{鲁棒性与误差分析}

\begin{frame}{特征提取有效性}
  不同特征提取方式的特征空间分布：
  \begin{figure}
  	\centering
  	\begin{tabular}{ccc}
  		\includegraphics[width=0.3\textwidth]{./assets/feature_origin.png} &
  		\includegraphics[width=0.3\textwidth]{./assets/feature_hu.png} &
  		\includegraphics[width=0.3\textwidth]{./assets/feature_hog.png} 
  	\end{tabular}
  \end{figure}
\end{frame}

\begin{frame}{训练模型对噪声的适应能力}
	由于原始数据噪声较少，通过人为添加噪声探究其对ababoost拟合效果的影响
	
	(特征与噪声在学习器中的权重图，title弱学习器的分类权重)
\end{frame}

\begin{frame}{训练模型对噪声的适应能力}
  
  (最终得到的加权学习器的权重图，title最终模型的权重)
  
  (噪声环境对准确率的影响图,title噪声环境下分辨优质样本的准确率)
\end{frame}

\begin{frame}{训练模型对噪声的适应能力}
  \begin{figure}
  	\centering
  	\begin{tabular}{cc}
  		\includegraphics[width=0.45\textwidth]{./assets/all_noise_accuracy.png} &
  		\includegraphics[width=0.45\textwidth]{./assets/all_noise_f1.png} \\ 
  	\end{tabular}
  \end{figure}
\end{frame}

\begin{frame}{训练模型对噪声的适应能力}
  \textbf{噪声研究分析与结论}
  \begin{itemize}
	\item 随着迭代次数增多，噪声权重增大导致后续学习器逐渐关注训练集的噪声部分
	\item 实际训练中，更关注噪声的这部分学习器在最终得到的模型中权重很小
	\item 训练集噪声对模型训练的影响相对稳定，且无噪声测试集仍呈现较高的准确率
  \end{itemize}
\end{frame}

\begin{frame}{对不同风格测试集的泛化能力}
  \textbf{泛化原理}:
  \begin{itemize}
  	\item 通关特征提取忽略与分类无关的特征因素
  \end{itemize}
  \textbf{测试原理}:
  \begin{itemize}
  	\item 在测试集中引入特定扰动以评估模型的泛化能力
  \end{itemize}
  \textbf{实现过程}:
  \begin{itemize}
  	\item 调整训练模型的风格参数实现不同风格
  \end{itemize}
\end{frame}

\begin{frame}{对不同风格测试集的泛化能力}
  \begin{figure}
  	\centering
  	\begin{tabular}{ccc}
  		\includegraphics[width=0.3\textwidth]{./assets/shift_1.png} &
  		\includegraphics[width=0.3\textwidth]{./assets/shift_2.png} &
  		\includegraphics[width=0.3\textwidth]{./assets/shift_3.png} 
  	\end{tabular}
  \end{figure}
\end{frame}

\begin{frame}{对不同风格测试集的泛化能力}
  \textbf{风格扰动研究分析与结论}:
  \begin{itemize}
  	\item 加入不同扰动后，特征提取的效果明显
  	\item 合适的特征提取能够有效减少风格本身的影响
  	\item 最终呈现的模型准确率与无扰动数据差距很小
  \end{itemize}
\end{frame}

\begin{frame}{误差分析}
  \textbf{误差来源与影响}
    \begin{itemize}
	  \item 样本数量少，识别结果方差大，准确率波动较大
	  \item 特征提取展示降维较多，数据展示较为单一
	  \item 人工噪声无法模拟真实的误差环境,实验对比更多体现了模型的泛化能力
	\end{itemize}
\end{frame}

\begin{frame}{基于MNIST变体的鲁棒性分析}
	\textbf{实验结果}：
	\begin{itemize}
		\item \textbf{噪声适应能力较强}:
		噪声研究显示，训练模型能够有效识别噪声并在合理迭代次数下，能够减少噪声对模型的影响，总体分析得出训练模型对噪声的适应能力较强
		\item \textbf{泛化能力强}:
		风格扰动对模型训练的干扰不明显，数据能够直观展现模型的泛化能力强
	\end{itemize}
	\textbf{综合评价}:
	\begin{itemize}
		\item 训练模型虽然可以保证对纯净样本较高的准确率，但准确率随噪声增大而下降的现象仍然明显，训练时需要\textbf{尽量避免噪声干扰}
		\item 特征提取是加强泛化能力的关键，需要\textbf{选择合适的特征提取方式}强化模型的泛化能力
	\end{itemize}
\end{frame}

\begin{frame}[standout]
	谢谢大家！
\end{frame}

\end{document}
